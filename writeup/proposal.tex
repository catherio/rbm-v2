\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{setspace}

\title{Project Proposal:\\ V1- and V2-like representations in sparse networks trained on natural images}
\author{Catherine Olsson}

\begin{document}

\maketitle

When analyzing natural images, many different objective functions give rise to Gabor-like, V1-like filters in layer 1. What can be said about layer 2? Can the properties of second-layer filters in multi-layer networks trained on iamges provide any insight into visual area V2 in primates? I intend to reimplement the network described in Lee et al. (2007) and explore its properties.

\section*{Background: V1-like features}
A variety of different objective functions give rise to oriented Gabor-like filters, which share a resemblance with the sorts of filters found in primate visual area V1. Sparse coding (Olshausen and Field 1996) and Independent Components  Analysis (Bell and Sejnowski 1997) both represent unsupervised approaches which give rise to oriented gabor-like filters. Supervised approaches also give rise to these filters, including the sparse energy-based model approach of Ranzato et al. (2006), as well as the first layer of most modern Deep Neural Nets trained to perform image recognition on large datasets such as ImageNet (see Krizhevsky et al. 2012). All these approaches, except the supervised AlexNet-like approaches, include sparsity as a central constraint; some of these approaches do not give rise to Gabor-like filters if sparsity constraint is not enforced.

%Although these Gabor-like representations resemble primate V1 neuron selectivity in many ways, they generally do not contain a comparable breadth of spatial scales. Van Hataren and Ruderman (1998) observe that training networks on temporal sequences of images, rather than static images, yields filters at a broader range of spatial as well as temporal scales. 

\section*{Background: V2-like features in multi-layer networks}

So far, I have discovered two existing approachs, in which the authors compare the second layer of a multi-layer network with V2 filter properties. Hyvarinen et al. 2005 apply ICA to the outputs of a hand-crafted set of complex-cell-like filter outputs that span frequency bands. The result is a set of second-stage units which preserve the orientation tuning of their constituent filters, but span across frequency bands to produce edge-like filters. 

Lee et al. 2007 take a different approach, in which both sets of filters are learned. These authors construct two layers of a deep belief net, using a restricted Boltzmann machine (RBM) approach. They train the layers greedily. Moreover, they enforce an additional sparsity constraint, to encourage the learning of Gabor-like features. Their second layer contains units selective for colinear (“contour”) features as well as corners and junctions. 

\section*{Approach}

I propose to set out with a primary goal of reimplementing the two-layer sparse RBM described in Lee et al. 2007. This will entail the sub-task of implementing the one-layer version first. The paper provides the size, layout, and update rule for the network, but not the learning rate or initializations, so I will likely have to reference previous RBM papers and do experiments of my own to get it to work.

If I succeed at that, then I will attempt to characterize the distribution of layer-1-to-2 connections. Which layer 1 filters tend to get combined? Are there any regularities? One possible approach to this is to first fit approximate parameters to the layer 1 filters (orientation, position, scale, etc.), and then plot histograms to characterize which layer 1 filters tend to get pooled by layer 2 filters (e.g. typical orientation difference of the top few connections across layer 2 filters)

\section*{Remaining Questions}

\begin{itemize}
\item If sparseness plays such a key role in the emergence of Gabor-like encodings, then why do Deep Neural Nets trained on ImageNet without a specific sparsity constraint seem to contain them, too?

\item What is the difference between filters and basis functions, when visualizing networks such as these?

\item What are ``heavy-tailed models'' (Osindero, Welling, Hinton 2006)?
\end{itemize}

\section*{Resources}
Works to reference:
\\
\singlespace
\leftskip 0.5in
\parindent -0.5in


Bell, A. and Sejnowsky, T. (1997). The ``Independent Components'' of Natural Scenes are Edge Filters. \emph{Vision Research, 37}(23), 3327-3338.

Krizhevsky, A., Sutskever, I., and Hinton, G. Classification with Deep Convolutional Neural Networks. (2012). \emph{Advances in Neural Information Processing Systems}(25).

Lee, H., Ekanadham, C., and Ng, A. (2007). Sparse Deep Belief Net Model for Visual Area V2. \emph{Advances in Neural Information Processing Systems}(20).

Ranzato, M., Poultney, C., Chopra, S., and LeCun, Y. (2006). Efficient Learning of Sparse Representations with an Energy-Based Model. \emph{Advances in Neural Information Processing Systems}(19).

van Hateren, J., and Ruderman. D. (1998). Independent component analysis of natural image sequences yields spatio-temporal filters similar to simple cells in primary visual cortex. \emph{Proc. R. Soc. Lond. B}(265), 2315-2320.

Hoyer, P., and Hyvärinen A. (2002). A multi-layer sparse coding network
learns contour coding from natural images. \emph{Vision Research, 42}(12), 1593-1605.

Hyvarinen, A., Gutmann, P., and Hoyer, P. (2005). Statistical model of natural stimuli predicts edge-like pooling of spatial frequency channels in V2. \emph{BMC Neuroscience, 6}(12).

Olshausen, B. and Field, J. (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. \emph{Nature}(381), 607-609

\end{document}
